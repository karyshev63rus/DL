{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# СТРОИМ НЕЙРОННУЮ СЕТЬ СО СКРЫТЫМ ReLU-СЛОЕМ ДЛЯ РАСПОЗНАВАНИЯ \n",
    "# РУКОПИСНЫХ ЦИФР\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#Sets the threshold for what messages will be logged.\n",
    "old_v = tf.logging.get_verbosity()\n",
    "\n",
    "# able to set the logging verbosity to either DEBUG, INFO, WARN,ERROR  \n",
    "# or FATAL. Here its ERROR\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# ... чтобы наша модель имела... название нейронной сети, нужно добавить\n",
    "# в нее скрытый слой. В качестве функции активации возьмем все тот же\n",
    "# ReLU. Нужно инициировать веса и свободный член для этого слоя;\n",
    "# будем в этом примере использовать 100 нейронов на скрытом слое:\n",
    "W_relu = tf.Variable(tf.truncated_normal([784,100], stddev=0.1))\n",
    "b_relu = tf.Variable(tf.truncated_normal([100], stddev=0.1))\n",
    "# На этот раз мы инициализируем переменные не нулями, а небольшими\n",
    "# случайными значениями. Функция tf.truncated_normal возвращает\n",
    "# значения, порожденные нормально распределенной случайной величиной\n",
    "# с фиксированными мат.ожиданием и дисперсией; однако при этом\n",
    "# значения, вышедшие за пределы итервала +-2сигма от среднего,\n",
    "# выбираются заново, то есть распределение обрезается так, чтобы\n",
    "# полностью запретить большие выбросы. Инициализация нулями в данном\n",
    "# случае была бы совсем бессмысленной, потому что ReLU(0)=0, а значит,\n",
    "# при инициализации нулями градиенты совсем не распространялись бы по сети.\n",
    "# В нашем же случае примерно половина весов окажется отрицательной, и   \n",
    "# соответствующие нейроны не будут активированы вовсе.\n",
    "\n",
    "# ... общий вид скрытого слоя получается таким:\n",
    "h = tf.nn.relu(tf.matmul(x,W_relu) + b_relu)\n",
    "# Применение tf.nn.relu тоже будет покомпонентным: фактически мы просто\n",
    "# применили функцию ReLU к вектору tf.matmul(x, W_relu) + b_relu\n",
    "\n",
    "# В этой нейронной сети будет очень полезен слой дропаута...: дропаут -\n",
    "# это слой, который выбрасывает (обнуляет) выходы некоторых нейронов,\n",
    "# выбираемых случайно и заново для каждого обучающего примера.\n",
    "# Все, что нам сейчас нужно задать - это вероятность их выбрасывания;\n",
    "# для этого мы сначала создадим заглушку:\n",
    "keep_probability = tf.placeholder(tf.float32)\n",
    "\n",
    "# А дальше TensorFlow... все делает за нас...\n",
    "h_drop = tf.nn.dropout(h, keep_probability)\n",
    "# Теперь нейроны скрытого слоя будут участвовать в вычислениях с вероятностью\n",
    "# keep_probability, а с вероятностью 1-keep_probability их выход будет \n",
    "# обнулен, и они не будут ни участвовать в предсказании для этого примера,\n",
    "# ни обучаться на нем. \n",
    "\n",
    "# Поскольку размер внутреннего слоя отличается от входного, нам придется\n",
    "# немного поменять параметры внешнего слоя и, кроме того, переписать\n",
    "# заключительный softmax-слой:\n",
    "#\n",
    "W = tf.Variable(tf.zeros([100, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.nn.softmax(tf.matmul(h_drop,W) + b)\n",
    "\n",
    "# [без изменений с предыдущей моделью - обучения логистической регрессии]\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    -tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "init = tf.initialize_all_variables()\n",
    "# При этом наша функция потель cross_entropy и оптимизатор train_step \n",
    "# не требуют никаких изменений. А вот вызвать sess.run нужноо с новым\n",
    "# переметром keep_probability. Поэтому цикл обучения немного изменился:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9639\n"
     ]
    }
   ],
   "source": [
    "# Вариант № 1: 2000 итераций с САМОПИСНОЙ функцией \n",
    "# расчета перекрестной энтропии\n",
    "\n",
    "# Если сделать 2000 шагов в обучении нашей новой сети, мы получим\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for i in range(2000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys,\n",
    "                                   keep_probability: 0.5})\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: %.4f\" %\n",
    "     sess.run(accuracy, feed_dict={x: mnist.test.images, \n",
    "                y_:mnist.test.labels, keep_probability: 1.}))\n",
    "# Отличный результат! Мы дописали еще несколько строк кода и с самыми\n",
    "# минимальными изменениями смогли добавить целый новый скрытый слой\n",
    "# в модель, а также уменьшить ошибку на тестовой выборке практически\n",
    "# в два раза."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0980\n"
     ]
    }
   ],
   "source": [
    "# Вариант № 2: 4000 итераций с САМОПИСНОЙ функцией \n",
    "# расчета перекрестной энтропии\n",
    "\n",
    "# Но и это еще не все. ... нельзя ли обучить нашу сеть еще лучше,\n",
    "# просто потратив на это больше времени? Но не тут-то было! Еще\n",
    "# через 2000 шагов обучения точность неожиданно резко падает\n",
    "# примерно до 10%\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for i in range(4000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys,\n",
    "                                   keep_probability: 0.5})\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: %.4f\" %\n",
    "     sess.run(accuracy, feed_dict={x: mnist.test.images, \n",
    "            y_:mnist.test.labels, keep_probability: 1.}))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Давайте разбираться, в чем причина. Вообще говоря, 10% подозрительно\n",
    "# похоже на точность генератора случайных чисел при угадывании\n",
    "# равномерно распределенных 10 классов; поэтому первое, что нужно \n",
    "# проверить - не сломалась ли наша модель полностью. Для этого можно,\n",
    "# например, посмотреть на ее веса:\n",
    "sess.run(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# И действительно, элементы вектора свободных членов b перестали быть\n",
    "# числами. Чаще всего это говорит о переполнении. Оно происходит, когда\n",
    "# очень большие числа выходят за границы соответствующих диапазонов и\n",
    "# округляются до \"бесконечности\" или \"минус бесконечности\"; чаще всего\n",
    "# так получается, когда очень маленькие числа округляются до нуля,\n",
    "# а потом на этот ноль что-нибудь пытаются разделить. Но откуда\n",
    "# переполнение могло появиться у нас?\n",
    "# Одно из \"узких\" мест нашего кода - функция softmax.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9737\n"
     ]
    }
   ],
   "source": [
    "# Вариант № 3: 10 000 итераций со ВСТРОЕННОЙ функцией \n",
    "# расчета перекрестной энтропии\n",
    "\n",
    "# К счастью, создатели TensorFlow знали об этих трудностях и специально\n",
    "# подготовили функцию, которая обходим все подводные камни. Давайте\n",
    "# использовать вместо самописных функций. Зададим промежуточный тензор\n",
    "logit = tf.matmul(h_drop, W) + b\n",
    "\n",
    "# Теперь мы не будем сами применять softmax или считать перекрестную\n",
    "# энтропию, а воспользуемся готовой функцией:\n",
    "# [аргументы должны быть именованы, иначе не работает]\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits\n",
    "                               (logits=logit, labels=y_))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Больше ничего менять не надо; давайте снова запустим код, теперь уже \n",
    "# сделав 10 000 шагов обучения, и посмотрим на результат:\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys,\n",
    "                                   keep_probability: 0.5})\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: %.4f\" %\n",
    "     sess.run(accuracy, feed_dict={x: mnist.test.images, \n",
    "                y_:mnist.test.labels, keep_probability: 1.}))\n",
    "# Неплохо! Уже 97,4% - теперь наша модель ошибается (на тестовой выборке,\n",
    "# естественно) всего один раз из сорока. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
